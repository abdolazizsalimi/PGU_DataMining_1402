{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Abdolazizsalimi980292626Ex6\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Dataset\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 colour images divided into 10 classes, with 6,000 images per class. The dataset is further divided into training and test sets, with 50,000 training images and 10,000 test images.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Given an image, can we predict the correct class of this image?\n",
    "\n",
    "The images are very small (32x32) and by visualizing them you will notice how difficult it is to distinguish them even for a human.\n",
    "\n",
    "In this notebook we are going to build a CNN model that can classify images of various objects. We have 10 class of images:\n",
    "\n",
    "1. Airplane\n",
    "2. Automobile\n",
    "3. Bird\n",
    "4. Cat\n",
    "5. Deer\n",
    "6. Dog\n",
    "7. Frog\n",
    "8. Horse\n",
    "9. Ship\n",
    "10. Truck\n",
    "\n",
    "### Training and Test Batches\n",
    "\n",
    "The dataset is organized into five training batches and one test batch, each containing 10,000 images. The test batch comprises 1,000 randomly-selected images from each class, resulting in a total of 10,000 test images. \n",
    "\n",
    "In contrast, the training batches contain the remaining images and are organized in random order. It is important to note that some training batches may have an uneven distribution of images across classes, with more images from one class than another. However, collectively, the training batches contain exactly 5,000 images from each class.\n",
    "\n",
    "This division allows researchers and practitioners to train machine learning models on the training set and evaluate their performance on the test set, providing a standardized benchmark for image classification tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.invalidate_caches()\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T14:43:51.492390200Z",
     "start_time": "2023-06-09T14:43:51.469008300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T14:43:54.211601100Z",
     "start_time": "2023-06-09T14:43:54.181095600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T14:43:55.986882900Z",
     "start_time": "2023-06-09T14:43:55.954217400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T14:43:58.604428300Z",
     "start_time": "2023-06-09T14:43:58.565346800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1024  # The default batch size of keras.\n",
    "# num_classes = 10  # Number of class for the dataset\n",
    "# epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels of the dataset\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "          'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Let's view more images in a grid format\n",
    "# Define the dimensions of the plot grid \n",
    "W_grid = 4\n",
    "L_grid = 4\n",
    "\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(L_grid, W_grid, figsize = (16 ,10))\n",
    "\n",
    "axes = axes.ravel() # flaten the 15 x 15 matrix into 225 array\n",
    "\n",
    "n_train = len(X_train) # get the length of the train dataset\n",
    "\n",
    "# Select a random number from 0 to n_train\n",
    "for i in np.arange(0, W_grid * L_grid): # create evenly spaces variables \n",
    "\n",
    "    # Select a random number\n",
    "    index = np.random.randint(0, n_train)\n",
    "    # read and display an image with the selected index    \n",
    "    axes[i].imshow(X_train[index,1:])\n",
    "    label_index = int(y_train[index])\n",
    "    axes[i].set_title(labels[label_index], fontsize = 8)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_name = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Count plot for training set\n",
    "sns.countplot(y_train.ravel(), ax=axs[0])\n",
    "axs[0].set_title('Distribution of Training Data')\n",
    "axs[0].set_xlabel('Classes')\n",
    "axs[0].set_ylabel('Count')\n",
    "axs[0].set_xticklabels(classes_name)\n",
    "\n",
    "# Count plot for testing set\n",
    "sns.countplot(y_test.ravel(), ax=axs[1])\n",
    "axs[1].set_title('Distribution of Testing Data')\n",
    "axs[1].set_xlabel('Classes')\n",
    "axs[1].set_ylabel('Count')\n",
    "axs[1].set_xticklabels(classes_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transform target variable into one-hot-encoding\n",
    "y_cat_train = to_categorical(y_train, 10)\n",
    "y_cat_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing The  Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "# Pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# Dropout layers\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "visualkeras.layered_view(model, scale_xy=10, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train = X_train.astype('float32')\n",
    "x_test = X_test.astype('float32')\n",
    "\n",
    "# Normalizing the input image\n",
    "import numpy as np\n",
    "\n",
    "# Compute mean and standard deviation of the training set\n",
    "mean = np.mean(x_train, axis=(0, 1, 2))\n",
    "std = np.std(x_train, axis=(0, 1, 2))\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
    "batch_size = 32\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "train_generator = data_generator.flow(x_train, y_cat_train, batch_size)\n",
    "steps_per_epoch = x_train.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_generator, \n",
    "              epochs=36,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_data=(x_test, y_cat_test), \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history\n",
    "df=pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['accuracy','val_accuracy']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['loss','val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "model.evaluate(x_test, y_cat_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=np.argmax(model.predict(x_test),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes=[np.argmax(element) for element in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_which_category(x, y, index):\n",
    "    print(\"predict to be\", classes_name[y_classes[index]])\n",
    "    plt.figure(figsize = (15,2))\n",
    "    plt.imshow(x[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_which_category(x_test,y_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_which_category(x_test,y_test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('best-CNN-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = r'./best-CNN-model.h5'\n",
    "model_path2 = r'./best_CNN-model.h5'\n",
    "\n",
    "# Load the model with the custom optimizer\n",
    "\n",
    "model = load_model(model_path)\n",
    "model = load_model(model_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
